{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2163a3b7-4279-4230-a11f-06fd0a2b77c8",
   "metadata": {},
   "source": [
    "### Task Î™©Ìëú - Faster R-CNN backbone ÏÑ†ÌÉù Ïãú Ï£ºÏùò ÏÇ¨Ìï≠ ÌååÏïÖ\n",
    "#### 1) Í¥ÄÏ∏° Í≤∞Í≥º(ÏÇ¨Ïã§)\n",
    "- (ÌòÑÏû¨ ÏÑ∏ÌåÖ Í∏∞Ï§Ä) VGG16 backbone + Faster R-CNNÏóêÏÑú\n",
    "- 1 epoch(750 iters) ‚âà 12ÏãúÍ∞Ñ ÏÜåÏöî ‚Üí 5 epoch ‚âà 60ÏãúÍ∞Ñ(ÏïΩ 2Ïùº 12ÏãúÍ∞Ñ)\n",
    "- Î∞òÎ©¥ EfficientNet-B0 backbone + Faster R-CNN Ïã§ÌóòÏùÄ ÏÉÅÎåÄÏ†ÅÏúºÎ°ú Îπ†Î•¥Í≤å ÌïôÏäµÏù¥ ÏßÑÌñâÎêòÏóàÍ≥†, AP50 ÏÑ±Îä•ÎèÑ 0.75 Ïù¥ÏÉÅ ÎèÑÎã¨Ìï®(Ïòà: epoch 14ÏóêÏÑú AP50‚âà0.762).\n",
    "#### 2) Í≤∞Î°†(ÌåêÎã®)\n",
    "- ÎèôÏùº ÏûêÏõê/ÌôòÍ≤ΩÏóêÏÑú Îπ†Î•∏ Ïã§Ìóò Î∞òÎ≥µÏù¥ ÌïÑÏöîÌïú ÏÉÅÌô©Ïù¥ÎùºÎ©¥, VGG16ÏùÑ Faster R-CNN backboneÏúºÎ°ú Ïì∞Îäî Í≤ÉÏùÄ ÎπÑÏö© ÎåÄÎπÑ ÎπÑÌö®Ïú®Ï†ÅÏùº Í∞ÄÎä•ÏÑ±Ïù¥ ÌÅº.\n",
    "- EfficientNet Í≥ÑÏó¥ backboneÏù¥ ÏÜçÎèÑ/ÏÑ±Îä• Î©¥ÏóêÏÑú Ïú†Î¶¨Ìïú ÌõÑÎ≥¥Í∞Ä Îê† Ïàò ÏûàÏùå.\n",
    "### EfficientNetÏù¥ Ïú†Î¶¨ÌñàÎçò Ïù¥Ïú†(Í∞ÄÏÑ§/ÏõêÏù∏ ÌõÑÎ≥¥ ‚Äî Ï∂îÍ∞Ä Í≤ÄÏ¶ù ÌïÑÏöî)\n",
    "#### (Í∞ÄÏÑ§ 1) AMP ÏÇ¨Ïö© Ïó¨Î∂Ä Ï∞®Ïù¥\n",
    "##### EfficientNet Ïã§Ìóò ÏΩîÎìúÏóêÎäî AMPÍ∞Ä Ìè¨Ìï®Îê®:\n",
    "- USE_AMP = (DEVICE.type == \"cuda\")\n",
    "- with torch.amp.autocast(\"cuda\", enabled=USE_AMP):\n",
    "- scaler = torch.amp.GradScaler(...)\n",
    "##### VGG Ïã§Ìóò ÏΩîÎìúÏóêÎäî AMPÍ∞Ä ÏóÜÏóàÏùå ‚Üí ÌòºÌï©Ï†ïÎ∞Ä Ï†ÅÏö© Ïú†Î¨¥Í∞Ä iteration timeÏóê ÌÅ∞ Ï∞®Ïù¥Î•º ÎßåÎì§ÏóàÏùÑ Í∞ÄÎä•ÏÑ±.\n",
    "#### (Í∞ÄÏÑ§ 2) Îç∞Ïù¥ÌÑ∞ Î°úÎî©/ÎîîÏΩîÎî© Î∞©Ïãù Ï∞®Ïù¥\n",
    "##### EfficientNet ÏΩîÎìú: Dataset Î°úÎî© Ïãú Ïù¥ÎØ∏ÏßÄÎ•º ÎØ∏Î¶¨ Ïó¥Ïñ¥(PIL) Î©îÎ™®Î¶¨Ïóê Î≥¥Í¥Ä(preload)\n",
    "- image = Image.open(img_path).convert(\"RGB\")\n",
    "- data.append((image, target))\n",
    "- VGG ÏΩîÎìú: __getitem__ÏóêÏÑú Îß§ iterationÎßàÎã§ Ïù¥ÎØ∏ÏßÄ open/ÎîîÏΩîÎî©(lazy loading)\n",
    "##### ‚Üí (ÌäπÌûà num_workers=0 ÌôòÍ≤ΩÏóêÏÑú) CPU ÎîîÏΩîÎî©/IO Î≥ëÎ™©ÏúºÎ°ú GPU ÎåÄÍ∏∞ ÏãúÍ∞ÑÏù¥ Ï¶ùÍ∞ÄÌñàÏùÑ Í∞ÄÎä•ÏÑ±.\n",
    "#### (Í∞ÄÏÑ§ 3) backbone ÏïÑÌÇ§ÌÖçÏ≤ò Ìö®Ïú® Ï∞®Ïù¥\n",
    "##### EfficientNet-B0Îäî Ïó∞ÏÇ∞ Ìö®Ïú®Ïù¥ ÎÜíÏùÄ Íµ¨Ï°∞(Î™®Î∞îÏùº/Í≤ΩÎüâ ÏßÄÌñ•)Ïù∏ Î∞òÎ©¥\n",
    "##### VGG16ÏùÄ Íµ¨Ìòï Íµ¨Ï°∞Î°ú Ïó∞ÏÇ∞ Ìö®Ïú®Ïù¥ ÎÇÆÏùÄ Ìé∏\n",
    "##### ‚Üí ÎèôÏùº detectorÏóêÏÑú stepÎãπ Ïó∞ÏÇ∞Îüâ Ï∞®Ïù¥Í∞Ä ÎàÑÏ†ÅÎêòÏóàÏùÑ Í∞ÄÎä•ÏÑ±."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "421a36cb-f680-47c2-86c4-59c72b1aa2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PyTorch 2.6.0+cu126 ÎåÄÏùë Î≤ÑÏ†Ñ\n",
    "# Faster R-CNN (GPU) + EfficientNet-B0 backbone\n",
    "# Î™©Ìëú: COCOeval AP@0.50(AP50) Í∏∞Ï§Ä\n",
    "#  - AP50 >= 0.75 : pass Î™®Îç∏ Ï†ÄÏû•(Í≥ÑÏÜç ÌïôÏäµ)\n",
    "#  - AP50 >= 0.80 : Î™®Îç∏ Ï†ÄÏû• + Ï¶âÏãú Ï¢ÖÎ£å\n",
    "#\n",
    "# DataLoader: num_workers=0 (Windows/Jupyter ÏïàÏ†ï)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms, models, ops\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection import rpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6a44743-fe96-435e-809f-eac9481a6f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.6.0+cu126\n",
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 0) Í∏∞Î≥∏ ÏÑ§Ï†ï\n",
    "# ---------------------------\n",
    "ROOT = \"../datasets/coco\"     # <- Îç∞Ïù¥ÌÑ∞ Î£®Ìä∏\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 0              # <- ÏöîÏ≤≠ÌïòÏã† ÎåÄÎ°ú 0\n",
    "EPOCHS = 120\n",
    "EVAL_EVERY = 2\n",
    "TARGET_AP50 = 0.75\n",
    "STOP_AP50 = 0.80\n",
    "PATIENCE = 12\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_AMP = (DEVICE.type == \"cuda\")\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"DEVICE:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "007c79c2-fba9-421d-934d-4352f1988b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 1) Dataset (COCO -> contiguous label Îß§Ìïë Ìè¨Ìï®)\n",
    "# ---------------------------\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, root, train: bool, transform=None):\n",
    "        super().__init__()\n",
    "        split = \"train\" if train else \"val\"\n",
    "        ann_path = os.path.join(root, \"annotations\", f\"{split}_annotations.json\")\n",
    "        self.coco = COCO(ann_path)\n",
    "        self.image_dir = os.path.join(root, split)\n",
    "        self.transform = transform\n",
    "\n",
    "        # category_id(ÏõêÎ≥∏) -> contiguous(1..K), 0ÏùÄ background\n",
    "        cat_ids = sorted(list(self.coco.cats.keys()))\n",
    "        self.cat_id_to_contiguous = {cat_id: i + 1 for i, cat_id in enumerate(cat_ids)}\n",
    "        self.contiguous_to_cat_id = {v: k for k, v in self.cat_id_to_contiguous.items()}\n",
    "\n",
    "        # ÌëúÏãúÏö©: contiguous -> name\n",
    "        self.categories = {0: \"background\"}\n",
    "        for cat_id in cat_ids:\n",
    "            self.categories[self.cat_id_to_contiguous[cat_id]] = self.coco.cats[cat_id][\"name\"]\n",
    "\n",
    "        self.data = self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        data = []\n",
    "        for img_id in self.coco.imgs:\n",
    "            img_info = self.coco.loadImgs(img_id)[0]\n",
    "            file_name = img_info[\"file_name\"]\n",
    "            img_path = os.path.join(self.image_dir, file_name)\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "            anns = self.coco.loadAnns(self.coco.getAnnIds(img_id))\n",
    "            boxes, labels = [], []\n",
    "            for ann in anns:\n",
    "                if ann.get(\"iscrowd\", 0) == 1:\n",
    "                    continue\n",
    "                x, y, w, h = ann[\"bbox\"]  # COCO: [x,y,w,h]\n",
    "                if w <= 0 or h <= 0:\n",
    "                    continue\n",
    "                boxes.append([x, y, x + w, y + h])  # -> xyxy\n",
    "                labels.append(self.cat_id_to_contiguous[int(ann[\"category_id\"])])\n",
    "\n",
    "            target = {\n",
    "                \"image_id\": torch.tensor([img_id], dtype=torch.int64),\n",
    "                \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "                \"labels\": torch.tensor(labels, dtype=torch.int64),\n",
    "            }\n",
    "            data.append((image, target))\n",
    "        return data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, target = self.data[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def collator(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15a672af-2c89-4a47-954b-b1dce1fe1152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.06s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 2) Transform / DataLoader (num_workers=0)\n",
    "# ---------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.ConvertImageDtype(dtype=torch.float),\n",
    "])\n",
    "\n",
    "train_ds = COCODataset(ROOT, train=True, transform=transform)\n",
    "val_ds   = COCODataset(ROOT, train=False, transform=transform)\n",
    "\n",
    "PIN_MEMORY = torch.cuda.is_available()\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collator,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collator,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03224462-f7c9-4ef6-b654-48e092ff5a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 3) Model: Faster R-CNN + EfficientNet-B0 backbone\n",
    "# ---------------------------\n",
    "# EfficientNet-B0 features out_channels = 1280\n",
    "backbone = models.efficientnet_b0(\n",
    "    weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1\n",
    ").features\n",
    "backbone.out_channels = 1280\n",
    "\n",
    "# ÏûëÏùÄ Í∞ùÏ≤¥Ïóê Ïú†Î¶¨ÌïòÎèÑÎ°ù anchorÏóê ÏûëÏùÄ size Ìè¨Ìï®\n",
    "anchor_generator = rpn.AnchorGenerator(\n",
    "    sizes=((16, 32, 64, 128, 256),),\n",
    "    aspect_ratios=((0.5, 1.0, 2.0),),\n",
    ")\n",
    "\n",
    "roi_pooler = ops.MultiScaleRoIAlign(\n",
    "    featmap_names=[\"0\"],     # backboneÏù¥ Îã®Ïùº feature mapÏùÑ Î∞òÌôòÌïúÎã§Í≥† Í∞ÄÏ†ï\n",
    "    output_size=(7, 7),\n",
    "    sampling_ratio=2,\n",
    ")\n",
    "\n",
    "num_classes = 1 + len(train_ds.cat_id_to_contiguous)  # background + K\n",
    "model = FasterRCNN(\n",
    "    backbone=backbone,\n",
    "    num_classes=num_classes,\n",
    "    rpn_anchor_generator=anchor_generator,\n",
    "    box_roi_pool=roi_pooler,\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f18471a1-a948-4dc5-ba13-dd1d67a2e9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 4) Optimizer / Scheduler / AMP\n",
    "# ---------------------------\n",
    "from torch import optim\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(params, lr=0.002, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.1)\n",
    "\n",
    "# PyTorch 2.6 Í∂åÏû• AMP API\n",
    "scaler = torch.amp.GradScaler(\"cuda\", enabled=USE_AMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ba9fca2-be36-4358-a04e-226783c92f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 5) COCOeval AP50 ÌèâÍ∞Ä Ìï®Ïàò (ÌïôÏäµ Ï§ëÏóî summarize Ï∂úÎ†• X)\n",
    "# ---------------------------\n",
    "import io\n",
    "import contextlib\n",
    "import numpy as np\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "def evaluate_ap50(\n",
    "    model,\n",
    "    dataloader,\n",
    "    dataset,\n",
    "    verbose: bool = False,\n",
    "    topk: int = 300,           # ‚úÖ Ïù¥ÎØ∏ÏßÄÎãπ ÏÉÅÏúÑ NÍ∞úÎßå ÌèâÍ∞Ä(ÏÜçÎèÑ ÌïµÏã¨)\n",
    "    score_thresh: float = 0.0  # ‚úÖ ÌïÑÏöîÏãú 0.05~0.2 Îì±ÏúºÎ°ú Ïò¨Î¶¨Î©¥ Îçî Îπ®ÎùºÏßê\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Faster R-CNN Ï∂úÎ†• -> COCOeval AP50 Í≥ÑÏÇ∞\n",
    "    - ÏÜçÎèÑ ÏµúÏ†ÅÌôî:\n",
    "      1) score_threshÎ°ú 1Ï∞® ÌïÑÌÑ∞\n",
    "      2) topkÎ°ú Ïù¥ÎØ∏ÏßÄÎãπ ÏòàÏ∏° Í∞úÏàò Ï†úÌïú\n",
    "    - summarizeÎäî stats Ï±ÑÏö∞Í∏∞ ÏúÑÌï¥ Ìï≠ÏÉÅ Ìò∏Ï∂úÌïòÎêò, verbose=FalseÎ©¥ Ï∂úÎ†•Îßå Ïà®ÍπÄ\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    coco_dets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            images = [img.to(DEVICE, non_blocking=True) for img in images]\n",
    "            outputs = model(images)\n",
    "\n",
    "            for i in range(len(targets)):\n",
    "                image_id = int(targets[i][\"image_id\"].cpu().numpy()[0])\n",
    "\n",
    "                boxes = outputs[i][\"boxes\"].detach().cpu().numpy()    # xyxy\n",
    "                scores = outputs[i][\"scores\"].detach().cpu().numpy()\n",
    "                labels = outputs[i][\"labels\"].detach().cpu().numpy()  # contiguous\n",
    "\n",
    "                if boxes.shape[0] == 0:\n",
    "                    continue\n",
    "\n",
    "                # 1) score threshold ÌïÑÌÑ∞\n",
    "                if score_thresh > 0.0:\n",
    "                    keep = scores >= score_thresh\n",
    "                    boxes = boxes[keep]\n",
    "                    scores = scores[keep]\n",
    "                    labels = labels[keep]\n",
    "                    if boxes.shape[0] == 0:\n",
    "                        continue\n",
    "\n",
    "                # 2) topk Ï†úÌïú (Ï†êÏàò ÎÇ¥Î¶ºÏ∞®Ïàú)\n",
    "                if topk is not None and boxes.shape[0] > topk:\n",
    "                    order = np.argsort(-scores)[:topk]\n",
    "                    boxes = boxes[order]\n",
    "                    scores = scores[order]\n",
    "                    labels = labels[order]\n",
    "\n",
    "                # xyxy -> xywh\n",
    "                boxes_xywh = boxes.copy()\n",
    "                boxes_xywh[:, 2] = boxes_xywh[:, 2] - boxes_xywh[:, 0]\n",
    "                boxes_xywh[:, 3] = boxes_xywh[:, 3] - boxes_xywh[:, 1]\n",
    "\n",
    "                # detections append\n",
    "                for j in range(len(boxes_xywh)):\n",
    "                    cont = int(labels[j])\n",
    "                    coco_cat_id = int(dataset.contiguous_to_cat_id[cont])\n",
    "                    coco_dets.append([\n",
    "                        image_id,\n",
    "                        float(boxes_xywh[j, 0]), float(boxes_xywh[j, 1]),\n",
    "                        float(boxes_xywh[j, 2]), float(boxes_xywh[j, 3]),\n",
    "                        float(scores[j]),\n",
    "                        coco_cat_id\n",
    "                    ])\n",
    "\n",
    "    # ÌÉêÏßÄ Í≤∞Í≥ºÍ∞Ä ÏïÑÏòà ÏóÜÏúºÎ©¥ AP50=0\n",
    "    if len(coco_dets) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    coco_dets = np.asarray(coco_dets)\n",
    "    coco_gt = dataset.coco\n",
    "    coco_dt = coco_gt.loadRes(coco_dets)\n",
    "\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, iouType=\"bbox\")\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "\n",
    "    # stats Ï±ÑÏö∞Í∏∞ ÏúÑÌï¥ summarizeÎäî Ìï≠ÏÉÅ Ìò∏Ï∂ú (Ï∂úÎ†•Îßå Ï†úÏñ¥)\n",
    "    if verbose:\n",
    "        coco_eval.summarize()\n",
    "    else:\n",
    "        with contextlib.redirect_stdout(io.StringIO()):\n",
    "            coco_eval.summarize()\n",
    "\n",
    "    # AP@[IoU=0.50] = stats[1]\n",
    "    return float(coco_eval.stats[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e65550f-2bfb-4bb1-9916-b4f0b4514e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "Converting ndarray to lists...\n",
      "(827, 7)\n",
      "0/827\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.08s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.03s).\n",
      "epoch=002  AP50=0.5934\n",
      "Loading and preparing results...\n",
      "Converting ndarray to lists...\n",
      "(658, 7)\n",
      "0/658\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.08s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "epoch=004  AP50=0.6673\n",
      "Loading and preparing results...\n",
      "Converting ndarray to lists...\n",
      "(794, 7)\n",
      "0/794\n",
      "DONE (t=0.10s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.08s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "epoch=006  AP50=0.6896\n",
      "Loading and preparing results...\n",
      "Converting ndarray to lists...\n",
      "(656, 7)\n",
      "0/656\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.08s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.03s).\n",
      "epoch=008  AP50=0.7168\n",
      "Loading and preparing results...\n",
      "Converting ndarray to lists...\n",
      "(685, 7)\n",
      "0/685\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.08s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.03s).\n",
      "epoch=010  AP50=0.7410\n",
      "Loading and preparing results...\n",
      "Converting ndarray to lists...\n",
      "(666, 7)\n",
      "0/666\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.07s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.03s).\n",
      "epoch=012  AP50=0.7247\n",
      "Loading and preparing results...\n",
      "Converting ndarray to lists...\n",
      "(657, 7)\n",
      "0/657\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.09s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "epoch=014  AP50=0.7620\n",
      "Loading and preparing results...\n",
      "Converting ndarray to lists...\n",
      "(636, 7)\n",
      "0/636\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.07s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "epoch=016  AP50=0.7327\n",
      "Loading and preparing results...\n",
      "Converting ndarray to lists...\n",
      "(661, 7)\n",
      "0/661\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.07s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.03s).\n",
      "epoch=018  AP50=0.7594\n",
      "Loading and preparing results...\n",
      "Converting ndarray to lists...\n",
      "(652, 7)\n",
      "0/652\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.07s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.03s).\n",
      "epoch=020  AP50=0.7397\n",
      "Loading and preparing results...\n",
      "Converting ndarray to lists...\n",
      "(663, 7)\n",
      "0/663\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.06s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "epoch=022  AP50=0.7710\n",
      "Loading and preparing results...\n",
      "Converting ndarray to lists...\n",
      "(670, 7)\n",
      "0/670\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.07s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "epoch=024  AP50=0.7598\n",
      "Loading and preparing results...\n",
      "Converting ndarray to lists...\n",
      "(641, 7)\n",
      "0/641\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.07s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.03s).\n",
      "epoch=026  AP50=0.7520\n",
      "Loading and preparing results...\n",
      "Converting ndarray to lists...\n",
      "(613, 7)\n",
      "0/613\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.07s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "epoch=028  AP50=0.7637\n",
      "Loading and preparing results...\n",
      "Converting ndarray to lists...\n",
      "(611, 7)\n",
      "0/611\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.07s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.03s).\n",
      "epoch=030  AP50=0.7424\n",
      "Loading and preparing results...\n",
      "Converting ndarray to lists...\n",
      "(661, 7)\n",
      "0/661\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.07s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "epoch=032  AP50=0.7601\n",
      "Loading and preparing results...\n",
      "Converting ndarray to lists...\n",
      "(666, 7)\n",
      "0/666\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.07s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "epoch=034  AP50=0.7561\n",
      "Loading and preparing results...\n",
      "Converting ndarray to lists...\n",
      "(647, 7)\n",
      "0/647\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.07s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "epoch=036  AP50=0.7554\n",
      "Loading and preparing results...\n",
      "Converting ndarray to lists...\n",
      "(633, 7)\n",
      "0/633\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.07s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "epoch=038  AP50=0.7436\n",
      "Loading and preparing results...\n",
      "Converting ndarray to lists...\n",
      "(661, 7)\n",
      "0/661\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.07s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.03s).\n",
      "epoch=040  AP50=0.7625\n",
      "Loading and preparing results...\n",
      "Converting ndarray to lists...\n",
      "(644, 7)\n",
      "0/644\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.07s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "epoch=042  AP50=0.7585\n",
      "Loading and preparing results...\n",
      "Converting ndarray to lists...\n",
      "(647, 7)\n",
      "0/647\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.07s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.03s).\n",
      "epoch=044  AP50=0.7471\n",
      "Loading and preparing results...\n",
      "Converting ndarray to lists...\n",
      "(671, 7)\n",
      "0/671\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.07s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      "epoch=046  AP50=0.7527\n",
      "üõë Early stopping (no improvement)\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 6) Train: AP50Îßå Ï∂úÎ†•, 0.75 Ï†ÄÏû•, 0.80 Ï¢ÖÎ£å, patience Ï¢ÖÎ£å\n",
    "# ---------------------------\n",
    "best_ap50 = -1.0\n",
    "bad_count = 0\n",
    "pass_saved = False\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        images = [img.to(DEVICE, non_blocking=True) for img in images]\n",
    "        targets = [{k: v.to(DEVICE, non_blocking=True) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # PyTorch 2.6 Í∂åÏû• autocast API\n",
    "        with torch.amp.autocast(\"cuda\", enabled=USE_AMP):\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        scaler.scale(losses).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running += float(losses.detach().item())\n",
    "\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # ----- evaluation -----\n",
    "    if epoch % EVAL_EVERY != 0:\n",
    "        continue\n",
    "\n",
    "    ap50 = evaluate_ap50(model, val_loader, val_ds, verbose=False)\n",
    "    print(f\"epoch={epoch:03d}  AP50={ap50:.4f}\")\n",
    "\n",
    "    # 0.75: ÌÜµÍ≥º Ï†ÄÏû•Îßå (Î©àÏ∂îÏßÄ ÏïäÏùå)\n",
    "    if (ap50 >= TARGET_AP50) and (not pass_saved):\n",
    "        torch.save(model.state_dict(), \"pass_ap50_0.75.pt\")\n",
    "        pass_saved = True\n",
    "\n",
    "    # 0.80: Ï†ÄÏû• + Ï¶âÏãú Ï¢ÖÎ£å\n",
    "    if ap50 >= STOP_AP50:\n",
    "        torch.save(model.state_dict(), \"pass_ap50_0.80.pt\")\n",
    "        print(\"üéØ STOP(AP50>=0.80) reached -> saved & stop\")\n",
    "        break\n",
    "\n",
    "    # best Í∞±Ïã† + patience (0.80 Î™ª Í∞ÄÎ©¥ Ïó¨Í∏∞ÏÑú Ï¢ÖÎ£åÎê† Ïàò ÏûàÏùå)\n",
    "    if ap50 > best_ap50:\n",
    "        best_ap50 = ap50\n",
    "        bad_count = 0\n",
    "        torch.save(model.state_dict(), \"best_ap50.pt\")\n",
    "    else:\n",
    "        bad_count += 1\n",
    "        if bad_count >= PATIENCE:\n",
    "            print(\"üõë Early stopping (no improvement)\")\n",
    "            break\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58d58e63-4ff7-49d5-8eca-faacd01eacc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_result(coco_eval, target_ap50=0.75):\n",
    "    ap, ap50, ap75, aps, apm, apl = coco_eval.stats[:6]\n",
    "\n",
    "    status = \"PASS\" if ap50 >= target_ap50 else \"FAIL\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"COCO Evaluation Summary\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    print(f\"Status        : {status}\")\n",
    "    print(f\"Target AP50   : {target_ap50:.2f}\")\n",
    "    print(\"-\"*40)\n",
    "\n",
    "    print(f\"AP50          : {ap50:.3f} ({ap50*100:.1f}%)\")\n",
    "    print(f\"AP75          : {ap75:.3f} ({ap75*100:.1f}%)\")\n",
    "    print(f\"AP@[0.50:0.95]: {ap:.3f} ({ap*100:.1f}%)\")\n",
    "\n",
    "    print(\"-\"*40)\n",
    "    print(f\"Small Object  : {aps:.3f}\")\n",
    "    print(f\"Medium Object : {apm:.3f}\")\n",
    "    print(f\"Large Object  : {apl:.3f}\")\n",
    "    print(\"=\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76604945-0528-42e5-a5ad-f7f7fad5651c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "Converting ndarray to lists...\n",
      "(663, 7)\n",
      "0/663\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.15s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.02s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.374\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.771\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.294\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.043\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.330\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.425\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.444\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.510\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.510\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.070\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.442\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.579\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 7) (ÏÑ†ÌÉù) ÌïôÏäµ ÌõÑ Í≥µÏãù Ìëú(12Ï§Ñ) Ï∂úÎ†•ÏùÄ Ïó¨Í∏∞ÏÑúÎßå\n",
    "#    - ÌïÑÏöîÌï† ÎïåÎßå ÏïÑÎûò Îëê Ï§ÑÏùÑ Ïã§ÌñâÌïòÏÑ∏Ïöî.\n",
    "# ---------------------------\n",
    "model.load_state_dict(torch.load(\"best_ap50.pt\", map_location=DEVICE))\n",
    "_ = evaluate_ap50(model, val_loader, val_ds, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7890dd11-6d71-4ae8-aaf3-931ae6f2c511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "Converting ndarray to lists...\n",
      "(663, 7)\n",
      "0/663\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "============================================\n",
      "PASS  (Target AP50 ‚â• 0.75)\n",
      "--------------------------------------------\n",
      "AP50          : 0.771 (77.1%)\n",
      "--------------------------------------------\n",
      "AP small/med/large : 0.043 / 0.330 / 0.425\n",
      "============================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import io, contextlib\n",
    "import numpy as np\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "# 1) ÏòàÏÅòÍ≤å Ï∂úÎ†• Ìï®Ïàò (Í≤∞Í≥ºÎßå)\n",
    "def pretty_print_result(coco_eval, target_ap50=0.75):\n",
    "    ap, ap50, ap75, aps, apm, apl = coco_eval.stats[:6]\n",
    "    status = \"PASS\" if ap50 >= target_ap50 else \"‚ùå FAIL\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*44)\n",
    "    print(f\"{status}  (Target AP50 ‚â• {target_ap50:.2f})\")\n",
    "    print(\"-\"*44)\n",
    "    print(f\"AP50          : {ap50:.3f} ({ap50*100:.1f}%)\")\n",
    "#    print(f\"AP75          : {ap75:.3f} ({ap75*100:.1f}%)\")\n",
    "#    print(f\"mAP@[.50:.95] : {ap:.3f} ({ap*100:.1f}%)\")\n",
    "    print(\"-\"*44)\n",
    "    print(f\"AP small/med/large : {aps:.3f} / {apm:.3f} / {apl:.3f}\")\n",
    "    print(\"=\"*44 + \"\\n\")\n",
    "\n",
    "# 2) COCOeval \"Í∞ùÏ≤¥\"Î•º ÎßåÎì§Ïñ¥Ï£ºÎäî ÌèâÍ∞Ä Ìï®Ïàò (ÌõàÎ†®X)\n",
    "def build_coco_eval(\n",
    "    model, dataloader, dataset,\n",
    "    topk=300, score_thresh=0.0, silent=True\n",
    "):\n",
    "    model.eval()\n",
    "    coco_dets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            images = [img.to(DEVICE, non_blocking=True) for img in images]\n",
    "            outputs = model(images)\n",
    "\n",
    "            for i in range(len(targets)):\n",
    "                image_id = int(targets[i][\"image_id\"].cpu().numpy()[0])\n",
    "\n",
    "                boxes = outputs[i][\"boxes\"].detach().cpu().numpy()\n",
    "                scores = outputs[i][\"scores\"].detach().cpu().numpy()\n",
    "                labels = outputs[i][\"labels\"].detach().cpu().numpy()\n",
    "\n",
    "                if boxes.shape[0] == 0:\n",
    "                    continue\n",
    "\n",
    "                # threshold\n",
    "                if score_thresh > 0.0:\n",
    "                    keep = scores >= score_thresh\n",
    "                    boxes, scores, labels = boxes[keep], scores[keep], labels[keep]\n",
    "                    if boxes.shape[0] == 0:\n",
    "                        continue\n",
    "\n",
    "                # topk\n",
    "                if topk is not None and boxes.shape[0] > topk:\n",
    "                    order = np.argsort(-scores)[:topk]\n",
    "                    boxes, scores, labels = boxes[order], scores[order], labels[order]\n",
    "\n",
    "                # xyxy -> xywh\n",
    "                boxes_xywh = boxes.copy()\n",
    "                boxes_xywh[:, 2] -= boxes_xywh[:, 0]\n",
    "                boxes_xywh[:, 3] -= boxes_xywh[:, 1]\n",
    "\n",
    "                for j in range(len(boxes_xywh)):\n",
    "                    coco_cat_id = int(dataset.contiguous_to_cat_id[int(labels[j])])\n",
    "                    coco_dets.append([\n",
    "                        image_id,\n",
    "                        float(boxes_xywh[j, 0]), float(boxes_xywh[j, 1]),\n",
    "                        float(boxes_xywh[j, 2]), float(boxes_xywh[j, 3]),\n",
    "                        float(scores[j]),\n",
    "                        coco_cat_id\n",
    "                    ])\n",
    "\n",
    "    coco_dets = np.asarray(coco_dets)\n",
    "    coco_gt = dataset.coco\n",
    "    coco_dt = coco_gt.loadRes(coco_dets)\n",
    "\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, iouType=\"bbox\")\n",
    "\n",
    "    if silent:\n",
    "        with contextlib.redirect_stdout(io.StringIO()):\n",
    "            coco_eval.evaluate()\n",
    "            coco_eval.accumulate()\n",
    "            coco_eval.summarize()   # stats Ï±ÑÏö∞Í∏∞Ïö©(Ï∂úÎ†•ÏùÄ Ïà®ÍπÄ)\n",
    "    else:\n",
    "        coco_eval.evaluate()\n",
    "        coco_eval.accumulate()\n",
    "        coco_eval.summarize()\n",
    "\n",
    "    return coco_eval\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Ïã§Ìñâ (ÌõàÎ†® ÏïÑÎãò)\n",
    "# --------------------------\n",
    "# best Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞\n",
    "model.load_state_dict(torch.load(\"best_ap50.pt\", map_location=DEVICE))\n",
    "\n",
    "# ÌèâÍ∞Ä 1ÌöåÎ°ú coco_eval ÎßåÎì§Í∏∞\n",
    "coco_eval = build_coco_eval(\n",
    "    model, val_loader, val_ds,\n",
    "    topk=300, score_thresh=0.0, silent=True\n",
    ")\n",
    "\n",
    "# Í≤∞Í≥ºÏ∂úÎ†•\n",
    "pretty_print_result(coco_eval, target_ap50=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a00e74-4748-41e6-8d7b-82f5a524ebf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch_first)",
   "language": "python",
   "name": "torch_first"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
